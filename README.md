# ğŸ“‰ Customer Churn Prediction â€“ End-to-End ML Pipeline

![Uploading churn  image.pngâ€¦]()


An end-to-end **Machine Learning system** that predicts telecom customer churn and exposes the model through a **FastAPI** service with a **Streamlit** dashboard for business users.

This project demonstrates the **full ML lifecycle**:

- Data ingestion & preprocessing  
- Model training and selection  
- Experiment tracking with MLflow  
- Model serving via REST API  
- Interactive UI for predictions & insights  
- (Optional) Dockerized architecture

---

## ğŸ¯ Problem Statement

Telecom companies lose significant revenue when customers discontinue their services (â€œchurnâ€).  
The goal of this project is to **predict the probability of churn for each customer**, so the business can:

- Identify **high-risk** customers  
- Design **retention campaigns**  
- Reduce **revenue loss** and improve **lifetime value**

---

## ğŸ§  Key Features

- âœ… End-to-end ML pipeline from raw data â†’ deployed model  
- âœ… Preprocessing & feature engineering using **Pandas + Scikit-learn**  
- âœ… Multiple models: **Logistic Regression, Random Forest, XGBoost**  
- âœ… Model selection based on **AUC, F1, Precision, Recall**  
- âœ… Experiment tracking with **MLflow**  
- âœ… Model persisted as a **single pickle pipeline** (`best_model.pkl`)  
- âœ… **FastAPI** for real-time prediction via `/predict` endpoint  
- âœ… **Streamlit dashboard** for:
  - Single-customer prediction
  - Basic churn analytics (distribution, churn vs contract, etc.)
- âœ… Designed for **Docker** (separate containers for API & dashboard)

---

## ğŸ— Architecture

**High-level flow:**

```text
Raw Data (CSV)
      â†“
Preprocessing & Feature Engineering (src/data_preprocessing.py)
      â†“
Train & Evaluate Models (src/train.py)
      â†“
Log Experiments (MLflow) & Save Best Model (models/best_model.pkl)
      â†“
Serve Model via FastAPI (api/main.py)
      â†“
Consume API from Streamlit Dashboard (dashboard/app.py)

```
## ğŸ“‚Project Structure
```
customer-churn-ml/
â”‚
â”œâ”€â”€ api/
â”‚   â””â”€â”€ main.py                # FastAPI app (prediction API)
â”‚
â”œâ”€â”€ dashboard/
â”‚   â””â”€â”€ app.py                 # Streamlit dashboard for UI & analytics
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                   # Raw dataset (Telco Customer Churn CSV)
â”‚   â””â”€â”€ processed/             # (Optional) processed data
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ best_model.pkl         # Trained best model (sklearn pipeline)
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ 01_eda_and_baseline.ipynb  # EDA and baseline model notebook
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py              # Paths & configuration
â”‚   â”œâ”€â”€ data_preprocessing.py  # Data loading & preprocessing pipeline
â”‚   â”œâ”€â”€ train.py               # Model training & MLflow logging
â”‚   â”œâ”€â”€ inference.py           # Model loading & prediction helper
â”‚   â”œâ”€â”€ schemas.py             # Pydantic schema for API input
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile.api         # Dockerfile for FastAPI service
â”‚   â””â”€â”€ Dockerfile.streamlit   # Dockerfile for Streamlit dashboard
â”‚
â”œâ”€â”€ docker-compose.yml         # (Optional) Compose setup for API + UI
â”œâ”€â”€ requirements.txt           # Project dependencies
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```
## ğŸ“Š Dataset
Source: Telco Customer Churn dataset (IBM Sample / Kaggle)

âš™ï¸ Setup & Installation (Local)
1ï¸âƒ£ Clone the repository
```
git clone https://github.com/rhanand01/customer-churn-ml-pipeline.git
cd customer-churn-ml-pipeline
```
2ï¸âƒ£ Create and activate virtual environment (optional but recommended)
```
python -m venv env
```
# Windows (PowerShell / CMD)
```
env\Scripts\activate
```
3ï¸âƒ£ Install dependencies
```
pip install --upgrade pip
pip install -r requirements.txt
```
4ï¸âƒ£ Place the dataset
Download the Telco Customer Churn CSV and place it in:
```
data/raw/WA_Fn-UseC_-Telco-Customer-Churn.csv

```
ğŸ¤– Training the Model
Run the training script:
```
python -m src.train
```
Running the FastAPI Service
```
From project root: uvicorn api.main:app --reload
```
Running the Streamlit Dashboard
With FastAPI running in one terminal, open another terminal (same env activated):
```
streamlit run dashboard/app.py

```
API will be available at:
Docs (Swagger):
```
 http://127.0.0.1:8000/docs
````
Health check: 
```
http://127.0.0.1:8000/health
